{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "#=============================================================================\n",
    "# SplitfedV2 (SFLV2) learning: ResNet18 on HAM10000\n",
    "# HAM10000 dataset: Tschandl, P.: The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions (2018), doi:10.7910/DVN/DBW86T\n",
    "\n",
    "# We have three versions of our implementations\n",
    "# Version1: without using socket and no DP+PixelDP\n",
    "# Version2: with using socket but no DP+PixelDP\n",
    "# Version3: without using socket but with DP+PixelDP\n",
    "\n",
    "# This program is Version1: Single program simulation\n",
    "# ==============================================================================\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from pandas import DataFrame\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from torchvision import datasets, models\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "# import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "# import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n",
    "\n",
    "# To print in color -------test/train of the client side\n",
    "def prRed(skk):\n",
    "    print(\"\\033[91m {}\\033[00m\".format(skk))\n",
    "\n",
    "def prGreen(skk):\n",
    "    print(\"\\033[92m {}\\033[00m\".format(skk))\n",
    "#=====================================================================================================\n",
    "#                           Client-side Model definition\n",
    "#=====================================================================================================\n",
    "# Model at client side\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes * self.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * self.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.bn1(self.conv1(x)))\n",
    "        out = torch.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = torch.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet50_client_side(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet50_client_side, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(BasicBlock, 64, 3, stride=1)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        return out\n",
    "\n",
    "#=====================================================================================================\n",
    "#                           Server-side Model definition\n",
    "#=====================================================================================================\n",
    "# Model at server side\n",
    "class ResNet50_server_side(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet50_server_side, self).__init__()\n",
    "        self.in_planes = 256  # 由于已经经过了self.layer1，所以更新in_planes\n",
    "\n",
    "        self.layer2 = self._make_layer(BasicBlock, 128, 4, stride=2)\n",
    "        self.layer3 = self._make_layer(BasicBlock, 256, 6, stride=2)\n",
    "        self.layer4 = self._make_layer(BasicBlock, 512, 3, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * BasicBlock.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer2(x)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "#====================================================================================================\n",
    "#                                  Server Side Programs\n",
    "#====================================================================================================\n",
    "# Federated averaging: FedAvg\n",
    "def FedAvg(w):\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    for k in w_avg.keys():\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[k] += w[i][k]\n",
    "        w_avg[k] = torch.div(w_avg[k], len(w))\n",
    "    return w_avg\n",
    "\n",
    "def calculate_accuracy(fx, y):\n",
    "    preds = fx.max(1, keepdim=True)[1]\n",
    "    correct = preds.eq(y.view_as(preds)).sum()\n",
    "    acc = 100.00 *correct.float()/preds.shape[0]\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "# Server-side function associated with Training\n",
    "def train_server(fx_client, y, l_epoch_count, l_epoch, idx, len_batch, net_glob_server, lr, criterion,\n",
    "                 batch_acc_train, batch_loss_train, count1, loss_train_collect_user, acc_train_collect_user, idx_collect,\n",
    "                 num_users, priority_queue, Server_FF_time_queue, BP_priority_queue, lock):\n",
    "    global l_epoch_check, fed_check\n",
    "    # global loss_train_collect, acc_train_collect\n",
    "    # global loss_train_collect_user, acc_train_collect_user\n",
    "    # print('idx_collect:', idx_collect)\n",
    "    # acc_avg_all_user_train = 0\n",
    "    # loss_avg_all_user_train = 0\n",
    "\n",
    "    net_glob_server.train()\n",
    "    optimizer_server = torch.optim.Adam(net_glob_server.parameters(), lr=lr)\n",
    "\n",
    "    # train and update\n",
    "    optimizer_server.zero_grad()\n",
    "\n",
    "    # print('client_fx type:', type(fx_client))\n",
    "    fx_client = fx_client\n",
    "    fx_client = fx_client.requires_grad_(True)\n",
    "    # print('client_fx type:', type(fx_client))\n",
    "    y = y\n",
    "    # print('y:', y)\n",
    "    # print('len(y):', len(y))\n",
    "\n",
    "    #---------forward prop-------------\n",
    "    fx_server = net_glob_server(fx_client)\n",
    "\n",
    "    # calculate loss\n",
    "    loss = criterion(fx_server, y)\n",
    "    # calculate accuracy\n",
    "    acc = calculate_accuracy(fx_server, y)\n",
    "\n",
    "    with lock:\n",
    "        del priority_queue[0]\n",
    "        del Server_FF_time_queue[0]\n",
    "    while True:\n",
    "        if BP_priority_queue[0] == idx:\n",
    "            break\n",
    "\n",
    "    #--------backward prop--------------\n",
    "    loss.backward()\n",
    "    dfx_client = fx_client.grad.clone().detach()\n",
    "    optimizer_server.step()\n",
    "\n",
    "    batch_loss_train.append(loss.item())\n",
    "    batch_acc_train.append(acc.item())\n",
    "\n",
    "    # server-side model net_glob_server is global so it is updated automatically in each pass to this function\n",
    "\n",
    "    # count1: to track the completion of the local batch associated with one client\n",
    "    # count1 += 1\n",
    "    # print('count1:', count1, '<===>len_batch:', len_batch)\n",
    "    if count1 == len_batch:\n",
    "        acc_avg_train = sum(batch_acc_train)/len(batch_acc_train)           # it has accuracy for one batch\n",
    "        loss_avg_train = sum(batch_loss_train)/len(batch_loss_train)\n",
    "\n",
    "        batch_acc_train = []\n",
    "        batch_loss_train = []\n",
    "        count1 = 0\n",
    "\n",
    "        prRed('Client{} Train => Local Epoch: {} \\tAcc: {:.3f} \\tLoss: {:.4f}'.format(idx, l_epoch_count, acc_avg_train, loss_avg_train))\n",
    "\n",
    "\n",
    "        # If one local epoch is completed, after this a new client will come\n",
    "        if l_epoch_count == l_epoch-1:\n",
    "\n",
    "            l_epoch_check = True                # to evaluate_server function - to check local epoch has completed or not\n",
    "\n",
    "            # we store the last accuracy in the last batch of the epoch and it is not the average of all local epochs\n",
    "            # this is because we work on the last trained model and its accuracy (not earlier cases)\n",
    "\n",
    "            #print(\"accuracy = \", acc_avg_train)\n",
    "            acc_avg_train_all = acc_avg_train\n",
    "            loss_avg_train_all = loss_avg_train\n",
    "\n",
    "            # accumulate accuracy and loss for each new user\n",
    "            loss_train_collect_user.append(loss_avg_train_all)\n",
    "            acc_train_collect_user.append(acc_avg_train_all)\n",
    "\n",
    "            # collect the id of each new user\n",
    "            if idx not in idx_collect:\n",
    "                idx_collect.append(idx)\n",
    "                #print(idx_collect)\n",
    "\n",
    "        # This is to check if all users are served for one round --------------------\n",
    "        if len(idx_collect) == num_users:\n",
    "            fed_check = True                                                  # to evaluate_server function  - to check fed check has hitted\n",
    "            # all users served for one round ------------------------- output print and update is done in evaluate_server()\n",
    "            # for nicer display\n",
    "\n",
    "            idx_collect = []\n",
    "\n",
    "            acc_avg_all_user_train = sum(acc_train_collect_user)/len(acc_train_collect_user)\n",
    "            loss_avg_all_user_train = sum(loss_train_collect_user)/len(loss_train_collect_user)\n",
    "\n",
    "            # loss_train_collect.append(loss_avg_all_user_train)\n",
    "            # acc_train_collect.append(acc_avg_all_user_train)\n",
    "\n",
    "            acc_train_collect_user = []\n",
    "            loss_train_collect_user = []\n",
    "\n",
    "    # send gradients to the client\n",
    "    # server_result_queue.put(dfx_client.to('cuda:'+str(idx)))\n",
    "    return dfx_client, net_glob_server\n",
    "\n",
    "# Server-side functions associated with Testing\n",
    "def evaluate_server(fx_client, y, idx, len_batch, ell):\n",
    "    global net_glob_server, criterion, batch_acc_test, batch_loss_test\n",
    "    global loss_test_collect, acc_test_collect, count2, num_users, acc_avg_train_all, loss_avg_train_all, l_epoch_check, fed_check\n",
    "    global loss_test_collect_user, acc_test_collect_user, acc_avg_all_user_train, loss_avg_all_user_train\n",
    "\n",
    "    net_glob_server.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fx_client = fx_client.to('cuda:'+str(idx))\n",
    "        y = y.to('cuda:'+str(idx))\n",
    "        #---------forward prop-------------\n",
    "        fx_server = net_glob_server(fx_client)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = criterion(fx_server, y)\n",
    "        # calculate accuracy\n",
    "        acc = calculate_accuracy(fx_server, y)\n",
    "\n",
    "\n",
    "        batch_loss_test.append(loss.item())\n",
    "        batch_acc_test.append(acc.item())\n",
    "\n",
    "\n",
    "        count2 += 1\n",
    "        if count2 == len_batch:\n",
    "            acc_avg_test = sum(batch_acc_test)/len(batch_acc_test)\n",
    "            loss_avg_test = sum(batch_loss_test)/len(batch_loss_test)\n",
    "\n",
    "            batch_acc_test = []\n",
    "            batch_loss_test = []\n",
    "            count2 = 0\n",
    "\n",
    "            print('Client{} Test =>                   \\tAcc: {:.3f} \\tLoss: {:.4f}'.format(idx, acc_avg_test, loss_avg_test))\n",
    "\n",
    "            # if a local epoch is completed\n",
    "            if l_epoch_check:\n",
    "                l_epoch_check = False\n",
    "\n",
    "                # Store the last accuracy and loss\n",
    "                acc_avg_test_all = acc_avg_test\n",
    "                loss_avg_test_all = loss_avg_test\n",
    "\n",
    "                loss_test_collect_user.append(loss_avg_test_all)\n",
    "                acc_test_collect_user.append(acc_avg_test_all)\n",
    "\n",
    "            # if all users are served for one round ----------\n",
    "            if fed_check:\n",
    "                fed_check = False\n",
    "\n",
    "                acc_avg_all_user = sum(acc_test_collect_user)/len(acc_test_collect_user)\n",
    "                loss_avg_all_user = sum(loss_test_collect_user)/len(loss_test_collect_user)\n",
    "\n",
    "                loss_test_collect.append(loss_avg_all_user)\n",
    "                acc_test_collect.append(acc_avg_all_user)\n",
    "                acc_test_collect_user = []\n",
    "                loss_test_collect_user= []\n",
    "\n",
    "                print(\"====================== SERVER V1==========================\")\n",
    "                print(' Train: Round {:3d}, Avg Accuracy {:.3f} | Avg Loss {:.3f}'.format(ell, acc_avg_all_user_train, loss_avg_all_user_train))\n",
    "                print(' Test: Round {:3d}, Avg Accuracy {:.3f} | Avg Loss {:.3f}'.format(ell, acc_avg_all_user, loss_avg_all_user))\n",
    "                print(\"==========================================================\")\n",
    "\n",
    "    return\n",
    "\n",
    "#==============================================================================================================\n",
    "#                                       Clients Side Program\n",
    "#==============================================================================================================\n",
    "class DatasetSplit(Dataset):\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = list(idxs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return image, label\n",
    "\n",
    "# Client-side functions associated with Training and Testing\n",
    "class Client(object):\n",
    "    def __init__(self, net_client_model, idx, lr, net_glob_server, criterion, count1, idx_collect, num_users, priority_queue, BP_priority_queue,\n",
    "                 Server_FF_time_queue, Server_BP_time_queue, client_BP_time_queue, client_FF_time_queue, w_glob_server_buffer, lock,\n",
    "                 dataset_train = None, dataset_test = None, idxs = None, idxs_test = None):\n",
    "        self.idx = idx\n",
    "        # self.device = device\n",
    "        self.lr = lr\n",
    "        self.local_ep = 1\n",
    "        self.net_glob_server = net_glob_server\n",
    "        self.criterion = criterion\n",
    "        self.batch_acc_train = []\n",
    "        self.batch_loss_train = []\n",
    "        self.loss_train_collect_user = []\n",
    "        self.acc_train_collect_user = []\n",
    "        self.count1 = 0\n",
    "        self.idx_collect = idx_collect\n",
    "        self.num_users = num_users\n",
    "        self.priority_queue = priority_queue\n",
    "        self.BP_priority_queue = BP_priority_queue\n",
    "        self.Server_FF_time_queue = Server_FF_time_queue\n",
    "        self.Server_BP_time_queue = Server_BP_time_queue\n",
    "        self.client_BP_time_queue = client_BP_time_queue\n",
    "        self.client_FF_time_queue = client_FF_time_queue\n",
    "        self.w_glob_server_buffer = w_glob_server_buffer\n",
    "        self.lock = lock\n",
    "        #self.selected_clients = []\n",
    "        self.ldr_train = DataLoader(DatasetSplit(dataset_train, idxs), batch_size = 256, shuffle = True)\n",
    "        self.ldr_test = DataLoader(DatasetSplit(dataset_test, idxs_test), batch_size = 256, shuffle = True)\n",
    "\n",
    "\n",
    "    def train(self, net):\n",
    "        net.train()\n",
    "        optimizer_client = torch.optim.Adam(net.parameters(), lr = self.lr)\n",
    "\n",
    "        for iter in range(self.local_ep):\n",
    "            len_batch = len(self.ldr_train)\n",
    "            for batch_idx, (images, labels) in enumerate(self.ldr_train):\n",
    "                FF_start_time = time.time()\n",
    "                images, labels = images.to('cuda:'+str(self.idx)), labels.to('cuda:3')\n",
    "\n",
    "                optimizer_client.zero_grad()\n",
    "                #---------forward prop-------------\n",
    "                fx = net(images)\n",
    "                client_fx = fx.clone().detach()\n",
    "\n",
    "                # transmit client_fx to server\n",
    "                client_fx = client_fx.to('cuda:3')\n",
    "\n",
    "                with self.lock:\n",
    "                    del self.client_FF_time_queue[self.idx]\n",
    "                    self.client_FF_time_queue.insert(self.idx, time.time() - FF_start_time)\n",
    "\n",
    "                    self.count1 = self.count1 + 1\n",
    "\n",
    "                    # insert new tasks\n",
    "                    if len(self.BP_priority_queue) == len(self.priority_queue):\n",
    "                        is_FF = True\n",
    "                    elif len(self.BP_priority_queue) > len(self.priority_queue) and len(self.priority_queue) > 0 and self.BP_priority_queue[0] == self.priority_queue[0]:\n",
    "                        is_FF = True\n",
    "                    else:\n",
    "                        is_FF = False\n",
    "\n",
    "                    if is_FF:\n",
    "                        if len(self.BP_priority_queue) == 0:\n",
    "                            self.BP_priority_queue.append(self.idx)\n",
    "                            self.Server_BP_time_queue.append(\n",
    "                                self.client_FF_time_queue[self.idx] + self.client_BP_time_queue[self.idx])\n",
    "                        else:\n",
    "                            for t in range(len(self.BP_priority_queue)):\n",
    "                                if self.client_FF_time_queue[self.idx] + self.client_BP_time_queue[self.idx] > \\\n",
    "                                        self.Server_BP_time_queue[t]:\n",
    "                                    self.BP_priority_queue.insert(t, self.idx)\n",
    "                                    self.Server_BP_time_queue.insert(t, self.client_FF_time_queue[self.idx] +\n",
    "                                                                     self.client_BP_time_queue[self.idx])\n",
    "                                    break\n",
    "                                elif t == len(self.BP_priority_queue) - 1:\n",
    "                                    self.BP_priority_queue.append(self.idx)\n",
    "                                    self.Server_BP_time_queue.append(\n",
    "                                        self.client_FF_time_queue[self.idx] + self.client_BP_time_queue[self.idx])\n",
    "\n",
    "                        if len(self.priority_queue) == 0 or len(self.priority_queue) == 1:\n",
    "                            self.priority_queue.append(self.idx)\n",
    "                            self.Server_FF_time_queue.append(\n",
    "                                self.client_FF_time_queue[self.idx] + self.client_BP_time_queue[self.idx])\n",
    "                        else:\n",
    "                            for t in range(1, len(self.priority_queue)):\n",
    "                                if self.client_FF_time_queue[self.idx] + self.client_BP_time_queue[self.idx] > \\\n",
    "                                        self.Server_FF_time_queue[t]:\n",
    "                                    self.priority_queue.insert(t, self.idx)\n",
    "                                    self.Server_FF_time_queue.insert(t, self.client_FF_time_queue[self.idx] +\n",
    "                                                                     self.client_BP_time_queue[self.idx])\n",
    "                                    break\n",
    "                                elif t == len(self.priority_queue) - 1:\n",
    "                                    self.priority_queue.append(self.idx)\n",
    "                                    self.Server_FF_time_queue.append(\n",
    "                                        self.client_FF_time_queue[self.idx] + self.client_BP_time_queue[self.idx])\n",
    "                    else:\n",
    "                        if len(self.BP_priority_queue) == 0 or len(self.BP_priority_queue) == 1:\n",
    "                            self.BP_priority_queue.append(self.idx)\n",
    "                            self.Server_BP_time_queue.append(\n",
    "                                self.client_FF_time_queue[self.idx] + self.client_BP_time_queue[self.idx])\n",
    "                        else:\n",
    "                            for t in range(1, len(self.BP_priority_queue)):\n",
    "                                if self.client_FF_time_queue[self.idx] + self.client_BP_time_queue[self.idx] > \\\n",
    "                                        self.Server_BP_time_queue[t]:\n",
    "                                    self.BP_priority_queue.insert(t, self.idx)\n",
    "                                    self.Server_BP_time_queue.insert(t, self.client_FF_time_queue[self.idx] +\n",
    "                                                                     self.client_BP_time_queue[self.idx])\n",
    "                                    break\n",
    "                                elif t == len(self.BP_priority_queue) - 1:\n",
    "                                    self.BP_priority_queue.append(self.idx)\n",
    "                                    self.Server_BP_time_queue.append(\n",
    "                                        self.client_FF_time_queue[self.idx] + self.client_BP_time_queue[self.idx])\n",
    "\n",
    "                        if len(self.priority_queue) == 0:\n",
    "                            self.priority_queue.append(self.idx)\n",
    "                            self.Server_FF_time_queue.append(\n",
    "                                self.client_FF_time_queue[self.idx] + self.client_BP_time_queue[self.idx])\n",
    "                        else:\n",
    "                            for t in range(len(self.priority_queue)):\n",
    "                                if self.client_FF_time_queue[self.idx] + self.client_BP_time_queue[self.idx] > \\\n",
    "                                        self.Server_FF_time_queue[t]:\n",
    "                                    self.priority_queue.insert(t, self.idx)\n",
    "                                    self.Server_FF_time_queue.insert(t, self.client_FF_time_queue[self.idx] +\n",
    "                                                                     self.client_BP_time_queue[self.idx])\n",
    "                                    break\n",
    "                                elif t == len(self.priority_queue) - 1:\n",
    "                                    self.priority_queue.append(self.idx)\n",
    "                                    self.Server_FF_time_queue.append(\n",
    "                                        self.client_FF_time_queue[self.idx] + self.client_BP_time_queue[self.idx])\n",
    "\n",
    "                # Client idx's task is started in Server when it has the highest priority\n",
    "                # print('priority_queue:', self.priority_queue)\n",
    "                while True:\n",
    "                    if len(self.priority_queue) >= 1:\n",
    "                        if self.priority_queue[0] == self.idx:\n",
    "                            # Sending activations to server and receiving gradients from server\n",
    "                            print('client ', self.idx, ' :', self.count1, '/', len_batch)\n",
    "                            dfx, net_glob_server = train_server(client_fx, labels, iter, self.local_ep, self.idx, len_batch, self.net_glob_server,\n",
    "                                               self.lr, self.criterion, self.batch_acc_train, self.batch_loss_train, self.count1,\n",
    "                                               self.loss_train_collect_user, self.acc_train_collect_user, self.idx_collect,\n",
    "                                               self.num_users, self.priority_queue, self.Server_FF_time_queue, self.BP_priority_queue, self.lock)\n",
    "\n",
    "                            with self.lock:\n",
    "                                del self.BP_priority_queue[0]\n",
    "                                del self.Server_BP_time_queue[0]\n",
    "                            break\n",
    "\n",
    "                #--------backward prop -------------\n",
    "                BP_start_time = time.time()\n",
    "                fx.backward(dfx.to('cuda:'+str(self.idx)))\n",
    "                optimizer_client.step()\n",
    "\n",
    "                with self.lock:\n",
    "                    del self.client_BP_time_queue[self.idx]\n",
    "                    self.client_BP_time_queue.insert(self.idx, time.time() - BP_start_time)\n",
    "\n",
    "            #prRed('Client{} Train => Epoch: {}'.format(self.idx, ell))\n",
    "        net.to('cpu')\n",
    "        net_glob_server.to('cpu')\n",
    "\n",
    "        return net.state_dict(), net_glob_server.state_dict()\n",
    "\n",
    "    def evaluate(self, net, ell):\n",
    "        net.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            len_batch = len(self.ldr_test)\n",
    "            for batch_idx, (images, labels) in enumerate(self.ldr_test):\n",
    "                images, labels = images.to('cuda:'+str(self.idx)), labels.to('cuda:'+str(self.idx))\n",
    "                #---------forward prop-------------\n",
    "                fx = net(images)\n",
    "\n",
    "                # Sending activations to server\n",
    "                evaluate_server(fx, labels, self.idx, len_batch, ell)\n",
    "\n",
    "            #prRed('Client{} Test => Epoch: {}'.format(self.idx, ell))\n",
    "\n",
    "        return\n",
    "\n",
    "#=====================================================================================================\n",
    "# dataset_iid() will create a dictionary to collect the indices of the data samples randomly for each client\n",
    "# IID HAM10000 datasets will be created based on this\n",
    "def dataset_iid(dataset, num_users):\n",
    "\n",
    "    num_items = int(len(dataset)/num_users)\n",
    "    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n",
    "    for i in range(num_users):\n",
    "        dict_users[i] = set(np.random.choice(all_idxs, num_items, replace = False))\n",
    "        all_idxs = list(set(all_idxs) - dict_users[i])\n",
    "    return dict_users\n",
    "\n",
    "def multiprocessing_train_and_test(local, idx, net_glob_client, net_glob_server, w_locals_client, w_glob_server_buffer):\n",
    "    net_glob_client.to('cuda:'+str(idx))\n",
    "    net_glob_server.to('cuda:3')\n",
    "    # with torch.cuda.device(idx):\n",
    "    # Training ------------------\n",
    "    w_client, w_glob_server = local.train(net=copy.deepcopy(net_glob_client).to('cuda:'+str(idx)))\n",
    "    # print('w_client type:', type(w_client))\n",
    "    # print(w_client.device.type)\n",
    "    w_locals_client.append(copy.deepcopy(w_client))\n",
    "\n",
    "    w_glob_server_buffer.append(copy.deepcopy(w_glob_server))\n",
    "\n",
    "    # Testing -------------------\n",
    "    # local.evaluate(net=copy.deepcopy(net_glob_client).to('cuda:'+str(idx)), ell=iter)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    torch.cuda.init()\n",
    "    torch.multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "    manager = multiprocessing.Manager()\n",
    "\n",
    "    SEED = 1234\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        print(torch.cuda.get_device_name(0))\n",
    "\n",
    "        # ===================================================================\n",
    "    program = \"PipeSFLV1 ResNet50 on Cifar100\"\n",
    "    print(f\"---------{program}----------\")  # this is to identify the program in the slurm outputs files\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('device:', device)\n",
    "\n",
    "    # ===================================================================\n",
    "    # No. of users\n",
    "    num_users = 3\n",
    "    epochs = 200\n",
    "    frac = 1  # participation of clients; if 1 then 100% clients participate in SFLV2\n",
    "    lr = 0.0001\n",
    "    train_times = []\n",
    "\n",
    "    net_glob_client = ResNet50_client_side()\n",
    "    print(net_glob_client)\n",
    "\n",
    "    # net_glob_server = manager.list()\n",
    "    # net_glob_server.append(ResNet18_server_side(Baseblock, [2, 2, 2], 100))\n",
    "    net_glob_server = ResNet50_server_side(100)  # 7 is my numbr of classes\n",
    "    print(net_glob_server)\n",
    "\n",
    "    # ===================================================================================\n",
    "    # For Server Side Loss and Accuracy\n",
    "    loss_train_collect = manager.list()\n",
    "    acc_train_collect = manager.list()\n",
    "    loss_test_collect = manager.list()\n",
    "    acc_test_collect = manager.list()\n",
    "    # batch_acc_train = manager.list()\n",
    "    # batch_loss_train = manager.list()\n",
    "    batch_acc_test = manager.list()\n",
    "    batch_loss_test = manager.list()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "\n",
    "    # to print train - test together in each round-- these are made global\n",
    "    # acc_avg_all_user_train = 0\n",
    "    # loss_avg_all_user_train = 0\n",
    "    # loss_train_collect_user = manager.list()\n",
    "    # acc_train_collect_user = manager.list()\n",
    "    loss_test_collect_user = manager.list()\n",
    "    acc_test_collect_user = manager.list()\n",
    "\n",
    "    # client idx collector\n",
    "    idx_collect = manager.list()\n",
    "    l_epoch_check = False\n",
    "    fed_check = False\n",
    "\n",
    "    # priority queue in server\n",
    "    priority_queue = manager.list()\n",
    "    BP_priority_queue = manager.list()\n",
    "    Server_FF_time_queue = manager.list()\n",
    "    Server_BP_time_queue = manager.list()\n",
    "    client_FF_time_queue = manager.list()\n",
    "    client_BP_time_queue = manager.list()\n",
    "    lock = manager.Lock()\n",
    "\n",
    "    # Init all clients' BP time is 0\n",
    "    for i in range(num_users):\n",
    "        client_FF_time_queue.append(0)\n",
    "        client_BP_time_queue.append(0)\n",
    "\n",
    "    #=============================================================================\n",
    "    #                         Data preprocessing\n",
    "    #=============================================================================\n",
    "    # Data preprocessing: Transformation\n",
    "    image_size = 32\n",
    "    normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(image_size, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "        ])\n",
    "    test_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "            ])\n",
    "\n",
    "    train_directory = os.path.join('cifar-100-python', 'train1')\n",
    "    valid_directory = os.path.join('cifar-100-python', 'val')\n",
    "    dataset_train = datasets.ImageFolder(root=train_directory, transform=train_transform)\n",
    "    dataset_test = datasets.ImageFolder(root=valid_directory, transform=test_transform)\n",
    "\n",
    "    #----------------------------------------------------------------\n",
    "    dict_users = dataset_iid(dataset_train, num_users)\n",
    "    dict_users_test = dataset_iid(dataset_test, num_users)\n",
    "\n",
    "    #------------ Training And Testing -----------------\n",
    "    net_glob_client.train()\n",
    "    #copy weights\n",
    "    w_glob_client = net_glob_client.state_dict()\n",
    "\n",
    "    # Federation takes place after certain local epochs in train() client-side\n",
    "    # this epoch is global epoch, also known as rounds\n",
    "\n",
    "    for iter in range(epochs):\n",
    "        start_time = time.time()\n",
    "        m = max(int(frac * num_users), 1)\n",
    "        idxs_users = np.random.choice(range(num_users), m, replace = False)\n",
    "        w_locals_client = manager.list()\n",
    "        w_glob_server_buffer = manager.list()\n",
    "        # net_glob_server.to('cpu')\n",
    "        processes = []\n",
    "\n",
    "        for idx in idxs_users:\n",
    "            local = Client(net_glob_client, idx, lr, net_glob_server, criterion, count1, idx_collect, num_users, priority_queue, BP_priority_queue,\n",
    "                           Server_FF_time_queue, Server_BP_time_queue, client_BP_time_queue, client_FF_time_queue, w_glob_server_buffer, lock, dataset_train = dataset_train,\n",
    "                           dataset_test = dataset_test, idxs = dict_users[idx], idxs_test = dict_users_test[idx])\n",
    "            process = multiprocessing.Process(target=multiprocessing_train_and_test, args=(local, idx, net_glob_client, net_glob_server, w_locals_client, w_glob_server_buffer), name=\"Client \"+str(idx))\n",
    "            processes.append(process)\n",
    "\n",
    "        # Start all clients for its local epochs------------\n",
    "        for p in processes:\n",
    "            p.start()\n",
    "\n",
    "        # After serving all clients for its local epochs------------\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "        idx_collect = manager.list()\n",
    "        # Federation process at Client-Side------------------------\n",
    "        print(\"------------------------------------------------------------\")\n",
    "        print(\"------ Fed Server: Federation process at Client-Side -------\")\n",
    "        print(\"------------------------------------------------------------\")\n",
    "        # w_glob_client.to('CPU')\n",
    "        print(len(w_locals_client))\n",
    "        # print(net_glob_server.state_dict())\n",
    "        # print(w_locals_client[0])\n",
    "\n",
    "        # w_locals_client_copy = list(w_locals_client)\n",
    "        # w_glob_client = FedAvg(w_locals_client_copy.to('CPU'))\n",
    "        w_glob_client = FedAvg(w_locals_client)\n",
    "        w_glob_server = FedAvg(w_glob_server_buffer)\n",
    "\n",
    "        # Update client-side global model\n",
    "        net_glob_client.load_state_dict(w_glob_client)\n",
    "\n",
    "        # Update server-side global model\n",
    "        net_glob_server.load_state_dict(w_glob_server)\n",
    "\n",
    "        train_time = time.time() - start_time  # 新增：计算当前轮次的训练时间\n",
    "        train_times.append(train_time)  # 新增：将当前轮次的训练时间添加到列表中\n",
    "\n",
    "        print(\"====================== PipeSFL V1 ========================\")\n",
    "        print('========== Train: Round {:3d} Time: {:2f}s ==============='.format(iter, train_time))\n",
    "        print(\"==========================================================\")\n",
    "    #===================================================================================\n",
    "\n",
    "    print(\"Training and Evaluation completed!\")\n",
    "\n",
    "    # 确保输出目录存在\n",
    "    curve_dir = 'result/curve'\n",
    "    model_dir = 'result/model'\n",
    "    acc_dir = 'result/acc'\n",
    "    loss_dir = 'result/loss'\n",
    "\n",
    "    for directory in [curve_dir, model_dir, acc_dir, loss_dir]:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "    # 绘制训练时间曲线\n",
    "    plt.plot(range(epochs), train_times)\n",
    "    plt.xlabel('Training Rounds')\n",
    "    plt.ylabel('Training Time (s)')\n",
    "    plt.title('Training Time Curve')\n",
    "    plt.grid(True)\n",
    "    # 保存图片 按照当前时间保存 目录为 output/curve\n",
    "    curve_filename = os.path.join(curve_dir,\n",
    "                                  'train_time_curve' + time.strftime(\"%Y%m%d%H%M%S\", time.localtime()) + '.png')\n",
    "    plt.savefig(curve_filename)\n",
    "\n",
    "    # 保存模型 命名为 模型名+当前时间\n",
    "    client_model_filename = os.path.join(model_dir, 'PipeSFLV1_ResNet50_Cifar100_Client' + time.strftime(\"%Y%m%d%H%M%S\",\n",
    "                                                                                                         time.localtime()) + '.pth')\n",
    "    server_model_filename = os.path.join(model_dir, 'PipeSFLV1_ResNet50_Cifar100_Server' + time.strftime(\"%Y%m%d%H%M%S\",\n",
    "                                                                                                         time.localtime()) + '.pth')\n",
    "    torch.save(net_glob_client.state_dict(), client_model_filename)\n",
    "    torch.save(net_glob_server.state_dict(), server_model_filename)\n",
    "    print('Model saved successfully!')\n",
    "\n",
    "    # 保存acc和loss数据\n",
    "    # acc_train_df = pd.DataFrame(acc_train_collect)\n",
    "    # loss_train_df = pd.DataFrame(loss_train_collect)\n",
    "    # acc_test_df = pd.DataFrame(acc_test_collect)\n",
    "    # loss_test_df = pd.DataFrame(loss_test_collect)\n",
    "\n",
    "    acc_train_collect_list = list(acc_train_collect)\n",
    "    loss_train_collect_list = list(loss_train_collect)\n",
    "    acc_test_collect_list = list(acc_test_collect)\n",
    "    loss_test_collect_list = list(loss_test_collect)\n",
    "\n",
    "    acc_train_df = pd.DataFrame(acc_train_collect_list)\n",
    "    loss_train_df = pd.DataFrame(loss_train_collect_list)\n",
    "    acc_test_df = pd.DataFrame(acc_test_collect_list)\n",
    "    loss_test_df = pd.DataFrame(loss_test_collect_list)\n",
    "\n",
    "    # 命名为 模型名+ 数据名+当前时间 目录为 output/acc\n",
    "    acc_train_filename = os.path.join(acc_dir, 'PipeSFLV1_ResNet50_Cifar100_Client_Acc' + time.strftime(\"%Y%m%d%H%M%S\",\n",
    "                                                                                                        time.localtime()) + '.csv')\n",
    "    acc_train_df.to_csv(acc_train_filename, index=False)\n",
    "    # 命名为 模型名+ 数据名+当前时间 目录为 output/loss\n",
    "    loss_train_filename = os.path.join(loss_dir,\n",
    "                                       'PipeSFLV1_ResNet50_Cifar100_Client_Loss' + time.strftime(\"%Y%m%d%H%M%S\",\n",
    "                                                                                                 time.localtime()) + '.csv')\n",
    "    loss_train_df.to_csv(loss_train_filename, index=False)\n",
    "    # 命名为 模型名+ 数据名+当前时间 目录为 output/acc\n",
    "    acc_test_filename = os.path.join(acc_dir, 'PipeSFLV1_ResNet50_Cifar100_Server_Acc' + time.strftime(\"%Y%m%d%H%M%S\",\n",
    "                                                                                                       time.localtime()) + '.csv')\n",
    "    acc_test_df.to_csv(acc_test_filename, index=False)\n",
    "    # 命名为 模型名+ 数据名+当前时间 目录为 output/loss\n",
    "    loss_test_filename = os.path.join(loss_dir,\n",
    "                                      'PipeSFLV1_ResNet50_Cifar100_Server_Loss' + time.strftime(\"%Y%m%d%H%M%S\",\n",
    "                                                                                                time.localtime()) + '.csv')\n",
    "    loss_test_df.to_csv(loss_test_filename, index=False)\n",
    "    print('Data saved successfully!')\n",
    "\n",
    "    os.system('/root/upload.sh')\n",
    "\n",
    "\n",
    "    #=============================================================================\n",
    "    #                         Program Completed\n",
    "    #=============================================================================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T04:53:59.780862Z",
     "start_time": "2025-03-04T04:53:59.479258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class BasicBlock(torch.nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(planes)\n",
    "        self.conv2 = torch.nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(planes)\n",
    "        self.conv3 = torch.nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = torch.nn.BatchNorm2d(planes * self.expansion)\n",
    "\n",
    "        self.shortcut = torch.nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes * self.expansion:\n",
    "            self.shortcut = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(in_planes, planes * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                torch.nn.BatchNorm2d(planes * self.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.bn1(self.conv1(x)))\n",
    "        out = torch.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = torch.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet50_client_side(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet50_client_side, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(BasicBlock, 64, 3, stride=1)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        return out\n",
    "\n",
    "class ResNet50_server_side(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet50_server_side, self).__init__()\n",
    "        self.in_planes = 256  # 由于已经经过了self.layer1，所以更新in_planes\n",
    "\n",
    "        self.layer2 = self._make_layer(BasicBlock, 128, 4, stride=2)\n",
    "        self.layer3 = self._make_layer(BasicBlock, 256, 6, stride=2)\n",
    "        self.layer4 = self._make_layer(BasicBlock, 512, 3, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * BasicBlock.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer2(x)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# evaluate .pth model\n",
    "\n",
    "# load the model\n",
    "server_model = ResNet50_server_side(100)\n",
    "client_model_path = \"output/model/PipeSFLV1_ResNet50_Cifar100_Client20250226184659.pth\"\n",
    "server_model_path = \"output/model/PipeSFLV1_ResNet50_Cifar100_Server20250226184659.pth\"\n",
    "\n",
    "client_model = ResNet50_client_side()\n",
    "client_model.load_state_dict(torch.load(client_model_path))\n",
    "server_model.load_state_dict(torch.load(server_model_path))\n",
    "\n",
    "\n",
    "# load the test data\n",
    "test_directory = os.path.join('data/cifar-100-python', 'val')\n",
    "test_dataset = datasets.ImageFolder(root=test_directory, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "total_acc = 0\n",
    "total_samples = 0\n",
    "total_loss = 0\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for images, labels in test_loader:\n",
    "#         images, labels = images.to('cuda:0'), labels.to('cuda:0')\n",
    "#         outputs = model(images)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         total_loss += loss.item()\n",
    "#         total_acc += (outputs.argmax(1) == labels).sum().item()\n",
    "#         total_samples += labels.size(0)\n",
    "\n",
    "avg_loss = total_loss / total_samples\n",
    "avg_acc = total_acc / total_samples\n",
    "\n",
    "print(f\"Test Loss: {avg_loss:.4f}, Test Acc: {avg_acc:.4f}\")"
   ],
   "id": "e614802d9f703c20",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNet50_client_side:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"layer1.0.conv1.weight\", \"layer1.0.bn1.weight\", \"layer1.0.bn1.bias\", \"layer1.0.bn1.running_mean\", \"layer1.0.bn1.running_var\", \"layer1.0.conv2.weight\", \"layer1.0.bn2.weight\", \"layer1.0.bn2.bias\", \"layer1.0.bn2.running_mean\", \"layer1.0.bn2.running_var\", \"layer1.0.conv3.weight\", \"layer1.0.bn3.weight\", \"layer1.0.bn3.bias\", \"layer1.0.bn3.running_mean\", \"layer1.0.bn3.running_var\", \"layer1.0.shortcut.0.weight\", \"layer1.0.shortcut.1.weight\", \"layer1.0.shortcut.1.bias\", \"layer1.0.shortcut.1.running_mean\", \"layer1.0.shortcut.1.running_var\", \"layer1.1.conv1.weight\", \"layer1.1.bn1.weight\", \"layer1.1.bn1.bias\", \"layer1.1.bn1.running_mean\", \"layer1.1.bn1.running_var\", \"layer1.1.conv2.weight\", \"layer1.1.bn2.weight\", \"layer1.1.bn2.bias\", \"layer1.1.bn2.running_mean\", \"layer1.1.bn2.running_var\", \"layer1.1.conv3.weight\", \"layer1.1.bn3.weight\", \"layer1.1.bn3.bias\", \"layer1.1.bn3.running_mean\", \"layer1.1.bn3.running_var\", \"layer1.2.conv1.weight\", \"layer1.2.bn1.weight\", \"layer1.2.bn1.bias\", \"layer1.2.bn1.running_mean\", \"layer1.2.bn1.running_var\", \"layer1.2.conv2.weight\", \"layer1.2.bn2.weight\", \"layer1.2.bn2.bias\", \"layer1.2.bn2.running_mean\", \"layer1.2.bn2.running_var\", \"layer1.2.conv3.weight\", \"layer1.2.bn3.weight\", \"layer1.2.bn3.bias\", \"layer1.2.bn3.running_mean\", \"layer1.2.bn3.running_var\". \n\tUnexpected key(s) in state_dict: \"layer2.0.conv1.weight\", \"layer2.0.bn1.weight\", \"layer2.0.bn1.bias\", \"layer2.0.bn1.running_mean\", \"layer2.0.bn1.running_var\", \"layer2.0.bn1.num_batches_tracked\", \"layer2.0.conv2.weight\", \"layer2.0.bn2.weight\", \"layer2.0.bn2.bias\", \"layer2.0.bn2.running_mean\", \"layer2.0.bn2.running_var\", \"layer2.0.bn2.num_batches_tracked\", \"layer2.0.conv3.weight\", \"layer2.0.bn3.weight\", \"layer2.0.bn3.bias\", \"layer2.0.bn3.running_mean\", \"layer2.0.bn3.running_var\", \"layer2.0.bn3.num_batches_tracked\", \"layer2.0.shortcut.0.weight\", \"layer2.0.shortcut.1.weight\", \"layer2.0.shortcut.1.bias\", \"layer2.0.shortcut.1.running_mean\", \"layer2.0.shortcut.1.running_var\", \"layer2.0.shortcut.1.num_batches_tracked\", \"layer2.1.conv1.weight\", \"layer2.1.bn1.weight\", \"layer2.1.bn1.bias\", \"layer2.1.bn1.running_mean\", \"layer2.1.bn1.running_var\", \"layer2.1.bn1.num_batches_tracked\", \"layer2.1.conv2.weight\", \"layer2.1.bn2.weight\", \"layer2.1.bn2.bias\", \"layer2.1.bn2.running_mean\", \"layer2.1.bn2.running_var\", \"layer2.1.bn2.num_batches_tracked\", \"layer2.1.conv3.weight\", \"layer2.1.bn3.weight\", \"layer2.1.bn3.bias\", \"layer2.1.bn3.running_mean\", \"layer2.1.bn3.running_var\", \"layer2.1.bn3.num_batches_tracked\", \"layer2.2.conv1.weight\", \"layer2.2.bn1.weight\", \"layer2.2.bn1.bias\", \"layer2.2.bn1.running_mean\", \"layer2.2.bn1.running_var\", \"layer2.2.bn1.num_batches_tracked\", \"layer2.2.conv2.weight\", \"layer2.2.bn2.weight\", \"layer2.2.bn2.bias\", \"layer2.2.bn2.running_mean\", \"layer2.2.bn2.running_var\", \"layer2.2.bn2.num_batches_tracked\", \"layer2.2.conv3.weight\", \"layer2.2.bn3.weight\", \"layer2.2.bn3.bias\", \"layer2.2.bn3.running_mean\", \"layer2.2.bn3.running_var\", \"layer2.2.bn3.num_batches_tracked\", \"layer2.3.conv1.weight\", \"layer2.3.bn1.weight\", \"layer2.3.bn1.bias\", \"layer2.3.bn1.running_mean\", \"layer2.3.bn1.running_var\", \"layer2.3.bn1.num_batches_tracked\", \"layer2.3.conv2.weight\", \"layer2.3.bn2.weight\", \"layer2.3.bn2.bias\", \"layer2.3.bn2.running_mean\", \"layer2.3.bn2.running_var\", \"layer2.3.bn2.num_batches_tracked\", \"layer2.3.conv3.weight\", \"layer2.3.bn3.weight\", \"layer2.3.bn3.bias\", \"layer2.3.bn3.running_mean\", \"layer2.3.bn3.running_var\", \"layer2.3.bn3.num_batches_tracked\", \"layer3.0.conv1.weight\", \"layer3.0.bn1.weight\", \"layer3.0.bn1.bias\", \"layer3.0.bn1.running_mean\", \"layer3.0.bn1.running_var\", \"layer3.0.bn1.num_batches_tracked\", \"layer3.0.conv2.weight\", \"layer3.0.bn2.weight\", \"layer3.0.bn2.bias\", \"layer3.0.bn2.running_mean\", \"layer3.0.bn2.running_var\", \"layer3.0.bn2.num_batches_tracked\", \"layer3.0.conv3.weight\", \"layer3.0.bn3.weight\", \"layer3.0.bn3.bias\", \"layer3.0.bn3.running_mean\", \"layer3.0.bn3.running_var\", \"layer3.0.bn3.num_batches_tracked\", \"layer3.0.shortcut.0.weight\", \"layer3.0.shortcut.1.weight\", \"layer3.0.shortcut.1.bias\", \"layer3.0.shortcut.1.running_mean\", \"layer3.0.shortcut.1.running_var\", \"layer3.0.shortcut.1.num_batches_tracked\", \"layer3.1.conv1.weight\", \"layer3.1.bn1.weight\", \"layer3.1.bn1.bias\", \"layer3.1.bn1.running_mean\", \"layer3.1.bn1.running_var\", \"layer3.1.bn1.num_batches_tracked\", \"layer3.1.conv2.weight\", \"layer3.1.bn2.weight\", \"layer3.1.bn2.bias\", \"layer3.1.bn2.running_mean\", \"layer3.1.bn2.running_var\", \"layer3.1.bn2.num_batches_tracked\", \"layer3.1.conv3.weight\", \"layer3.1.bn3.weight\", \"layer3.1.bn3.bias\", \"layer3.1.bn3.running_mean\", \"layer3.1.bn3.running_var\", \"layer3.1.bn3.num_batches_tracked\", \"layer3.2.conv1.weight\", \"layer3.2.bn1.weight\", \"layer3.2.bn1.bias\", \"layer3.2.bn1.running_mean\", \"layer3.2.bn1.running_var\", \"layer3.2.bn1.num_batches_tracked\", \"layer3.2.conv2.weight\", \"layer3.2.bn2.weight\", \"layer3.2.bn2.bias\", \"layer3.2.bn2.running_mean\", \"layer3.2.bn2.running_var\", \"layer3.2.bn2.num_batches_tracked\", \"layer3.2.conv3.weight\", \"layer3.2.bn3.weight\", \"layer3.2.bn3.bias\", \"layer3.2.bn3.running_mean\", \"layer3.2.bn3.running_var\", \"layer3.2.bn3.num_batches_tracked\", \"layer3.3.conv1.weight\", \"layer3.3.bn1.weight\", \"layer3.3.bn1.bias\", \"layer3.3.bn1.running_mean\", \"layer3.3.bn1.running_var\", \"layer3.3.bn1.num_batches_tracked\", \"layer3.3.conv2.weight\", \"layer3.3.bn2.weight\", \"layer3.3.bn2.bias\", \"layer3.3.bn2.running_mean\", \"layer3.3.bn2.running_var\", \"layer3.3.bn2.num_batches_tracked\", \"layer3.3.conv3.weight\", \"layer3.3.bn3.weight\", \"layer3.3.bn3.bias\", \"layer3.3.bn3.running_mean\", \"layer3.3.bn3.running_var\", \"layer3.3.bn3.num_batches_tracked\", \"layer3.4.conv1.weight\", \"layer3.4.bn1.weight\", \"layer3.4.bn1.bias\", \"layer3.4.bn1.running_mean\", \"layer3.4.bn1.running_var\", \"layer3.4.bn1.num_batches_tracked\", \"layer3.4.conv2.weight\", \"layer3.4.bn2.weight\", \"layer3.4.bn2.bias\", \"layer3.4.bn2.running_mean\", \"layer3.4.bn2.running_var\", \"layer3.4.bn2.num_batches_tracked\", \"layer3.4.conv3.weight\", \"layer3.4.bn3.weight\", \"layer3.4.bn3.bias\", \"layer3.4.bn3.running_mean\", \"layer3.4.bn3.running_var\", \"layer3.4.bn3.num_batches_tracked\", \"layer3.5.conv1.weight\", \"layer3.5.bn1.weight\", \"layer3.5.bn1.bias\", \"layer3.5.bn1.running_mean\", \"layer3.5.bn1.running_var\", \"layer3.5.bn1.num_batches_tracked\", \"layer3.5.conv2.weight\", \"layer3.5.bn2.weight\", \"layer3.5.bn2.bias\", \"layer3.5.bn2.running_mean\", \"layer3.5.bn2.running_var\", \"layer3.5.bn2.num_batches_tracked\", \"layer3.5.conv3.weight\", \"layer3.5.bn3.weight\", \"layer3.5.bn3.bias\", \"layer3.5.bn3.running_mean\", \"layer3.5.bn3.running_var\", \"layer3.5.bn3.num_batches_tracked\", \"layer4.0.conv1.weight\", \"layer4.0.bn1.weight\", \"layer4.0.bn1.bias\", \"layer4.0.bn1.running_mean\", \"layer4.0.bn1.running_var\", \"layer4.0.bn1.num_batches_tracked\", \"layer4.0.conv2.weight\", \"layer4.0.bn2.weight\", \"layer4.0.bn2.bias\", \"layer4.0.bn2.running_mean\", \"layer4.0.bn2.running_var\", \"layer4.0.bn2.num_batches_tracked\", \"layer4.0.conv3.weight\", \"layer4.0.bn3.weight\", \"layer4.0.bn3.bias\", \"layer4.0.bn3.running_mean\", \"layer4.0.bn3.running_var\", \"layer4.0.bn3.num_batches_tracked\", \"layer4.0.shortcut.0.weight\", \"layer4.0.shortcut.1.weight\", \"layer4.0.shortcut.1.bias\", \"layer4.0.shortcut.1.running_mean\", \"layer4.0.shortcut.1.running_var\", \"layer4.0.shortcut.1.num_batches_tracked\", \"layer4.1.conv1.weight\", \"layer4.1.bn1.weight\", \"layer4.1.bn1.bias\", \"layer4.1.bn1.running_mean\", \"layer4.1.bn1.running_var\", \"layer4.1.bn1.num_batches_tracked\", \"layer4.1.conv2.weight\", \"layer4.1.bn2.weight\", \"layer4.1.bn2.bias\", \"layer4.1.bn2.running_mean\", \"layer4.1.bn2.running_var\", \"layer4.1.bn2.num_batches_tracked\", \"layer4.1.conv3.weight\", \"layer4.1.bn3.weight\", \"layer4.1.bn3.bias\", \"layer4.1.bn3.running_mean\", \"layer4.1.bn3.running_var\", \"layer4.1.bn3.num_batches_tracked\", \"layer4.2.conv1.weight\", \"layer4.2.bn1.weight\", \"layer4.2.bn1.bias\", \"layer4.2.bn1.running_mean\", \"layer4.2.bn1.running_var\", \"layer4.2.bn1.num_batches_tracked\", \"layer4.2.conv2.weight\", \"layer4.2.bn2.weight\", \"layer4.2.bn2.bias\", \"layer4.2.bn2.running_mean\", \"layer4.2.bn2.running_var\", \"layer4.2.bn2.num_batches_tracked\", \"layer4.2.conv3.weight\", \"layer4.2.bn3.weight\", \"layer4.2.bn3.bias\", \"layer4.2.bn3.running_mean\", \"layer4.2.bn3.running_var\", \"layer4.2.bn3.num_batches_tracked\", \"fc.weight\", \"fc.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 67\u001B[0m\n\u001B[0;32m     64\u001B[0m model \u001B[38;5;241m=\u001B[39m ResNet50_client_side()\n\u001B[0;32m     65\u001B[0m model_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput/model/PipeSFLV1_ResNet50_Cifar100_Server20250226184659.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 67\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_path\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     68\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n\u001B[0;32m     70\u001B[0m \u001B[38;5;66;03m# load the test data\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:2189\u001B[0m, in \u001B[0;36mModule.load_state_dict\u001B[1;34m(self, state_dict, strict, assign)\u001B[0m\n\u001B[0;32m   2184\u001B[0m         error_msgs\u001B[38;5;241m.\u001B[39minsert(\n\u001B[0;32m   2185\u001B[0m             \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m   2186\u001B[0m                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)))\n\u001B[0;32m   2188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m-> 2189\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m   2190\u001B[0m                        \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(error_msgs)))\n\u001B[0;32m   2191\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for ResNet50_client_side:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"layer1.0.conv1.weight\", \"layer1.0.bn1.weight\", \"layer1.0.bn1.bias\", \"layer1.0.bn1.running_mean\", \"layer1.0.bn1.running_var\", \"layer1.0.conv2.weight\", \"layer1.0.bn2.weight\", \"layer1.0.bn2.bias\", \"layer1.0.bn2.running_mean\", \"layer1.0.bn2.running_var\", \"layer1.0.conv3.weight\", \"layer1.0.bn3.weight\", \"layer1.0.bn3.bias\", \"layer1.0.bn3.running_mean\", \"layer1.0.bn3.running_var\", \"layer1.0.shortcut.0.weight\", \"layer1.0.shortcut.1.weight\", \"layer1.0.shortcut.1.bias\", \"layer1.0.shortcut.1.running_mean\", \"layer1.0.shortcut.1.running_var\", \"layer1.1.conv1.weight\", \"layer1.1.bn1.weight\", \"layer1.1.bn1.bias\", \"layer1.1.bn1.running_mean\", \"layer1.1.bn1.running_var\", \"layer1.1.conv2.weight\", \"layer1.1.bn2.weight\", \"layer1.1.bn2.bias\", \"layer1.1.bn2.running_mean\", \"layer1.1.bn2.running_var\", \"layer1.1.conv3.weight\", \"layer1.1.bn3.weight\", \"layer1.1.bn3.bias\", \"layer1.1.bn3.running_mean\", \"layer1.1.bn3.running_var\", \"layer1.2.conv1.weight\", \"layer1.2.bn1.weight\", \"layer1.2.bn1.bias\", \"layer1.2.bn1.running_mean\", \"layer1.2.bn1.running_var\", \"layer1.2.conv2.weight\", \"layer1.2.bn2.weight\", \"layer1.2.bn2.bias\", \"layer1.2.bn2.running_mean\", \"layer1.2.bn2.running_var\", \"layer1.2.conv3.weight\", \"layer1.2.bn3.weight\", \"layer1.2.bn3.bias\", \"layer1.2.bn3.running_mean\", \"layer1.2.bn3.running_var\". \n\tUnexpected key(s) in state_dict: \"layer2.0.conv1.weight\", \"layer2.0.bn1.weight\", \"layer2.0.bn1.bias\", \"layer2.0.bn1.running_mean\", \"layer2.0.bn1.running_var\", \"layer2.0.bn1.num_batches_tracked\", \"layer2.0.conv2.weight\", \"layer2.0.bn2.weight\", \"layer2.0.bn2.bias\", \"layer2.0.bn2.running_mean\", \"layer2.0.bn2.running_var\", \"layer2.0.bn2.num_batches_tracked\", \"layer2.0.conv3.weight\", \"layer2.0.bn3.weight\", \"layer2.0.bn3.bias\", \"layer2.0.bn3.running_mean\", \"layer2.0.bn3.running_var\", \"layer2.0.bn3.num_batches_tracked\", \"layer2.0.shortcut.0.weight\", \"layer2.0.shortcut.1.weight\", \"layer2.0.shortcut.1.bias\", \"layer2.0.shortcut.1.running_mean\", \"layer2.0.shortcut.1.running_var\", \"layer2.0.shortcut.1.num_batches_tracked\", \"layer2.1.conv1.weight\", \"layer2.1.bn1.weight\", \"layer2.1.bn1.bias\", \"layer2.1.bn1.running_mean\", \"layer2.1.bn1.running_var\", \"layer2.1.bn1.num_batches_tracked\", \"layer2.1.conv2.weight\", \"layer2.1.bn2.weight\", \"layer2.1.bn2.bias\", \"layer2.1.bn2.running_mean\", \"layer2.1.bn2.running_var\", \"layer2.1.bn2.num_batches_tracked\", \"layer2.1.conv3.weight\", \"layer2.1.bn3.weight\", \"layer2.1.bn3.bias\", \"layer2.1.bn3.running_mean\", \"layer2.1.bn3.running_var\", \"layer2.1.bn3.num_batches_tracked\", \"layer2.2.conv1.weight\", \"layer2.2.bn1.weight\", \"layer2.2.bn1.bias\", \"layer2.2.bn1.running_mean\", \"layer2.2.bn1.running_var\", \"layer2.2.bn1.num_batches_tracked\", \"layer2.2.conv2.weight\", \"layer2.2.bn2.weight\", \"layer2.2.bn2.bias\", \"layer2.2.bn2.running_mean\", \"layer2.2.bn2.running_var\", \"layer2.2.bn2.num_batches_tracked\", \"layer2.2.conv3.weight\", \"layer2.2.bn3.weight\", \"layer2.2.bn3.bias\", \"layer2.2.bn3.running_mean\", \"layer2.2.bn3.running_var\", \"layer2.2.bn3.num_batches_tracked\", \"layer2.3.conv1.weight\", \"layer2.3.bn1.weight\", \"layer2.3.bn1.bias\", \"layer2.3.bn1.running_mean\", \"layer2.3.bn1.running_var\", \"layer2.3.bn1.num_batches_tracked\", \"layer2.3.conv2.weight\", \"layer2.3.bn2.weight\", \"layer2.3.bn2.bias\", \"layer2.3.bn2.running_mean\", \"layer2.3.bn2.running_var\", \"layer2.3.bn2.num_batches_tracked\", \"layer2.3.conv3.weight\", \"layer2.3.bn3.weight\", \"layer2.3.bn3.bias\", \"layer2.3.bn3.running_mean\", \"layer2.3.bn3.running_var\", \"layer2.3.bn3.num_batches_tracked\", \"layer3.0.conv1.weight\", \"layer3.0.bn1.weight\", \"layer3.0.bn1.bias\", \"layer3.0.bn1.running_mean\", \"layer3.0.bn1.running_var\", \"layer3.0.bn1.num_batches_tracked\", \"layer3.0.conv2.weight\", \"layer3.0.bn2.weight\", \"layer3.0.bn2.bias\", \"layer3.0.bn2.running_mean\", \"layer3.0.bn2.running_var\", \"layer3.0.bn2.num_batches_tracked\", \"layer3.0.conv3.weight\", \"layer3.0.bn3.weight\", \"layer3.0.bn3.bias\", \"layer3.0.bn3.running_mean\", \"layer3.0.bn3.running_var\", \"layer3.0.bn3.num_batches_tracked\", \"layer3.0.shortcut.0.weight\", \"layer3.0.shortcut.1.weight\", \"layer3.0.shortcut.1.bias\", \"layer3.0.shortcut.1.running_mean\", \"layer3.0.shortcut.1.running_var\", \"layer3.0.shortcut.1.num_batches_tracked\", \"layer3.1.conv1.weight\", \"layer3.1.bn1.weight\", \"layer3.1.bn1.bias\", \"layer3.1.bn1.running_mean\", \"layer3.1.bn1.running_var\", \"layer3.1.bn1.num_batches_tracked\", \"layer3.1.conv2.weight\", \"layer3.1.bn2.weight\", \"layer3.1.bn2.bias\", \"layer3.1.bn2.running_mean\", \"layer3.1.bn2.running_var\", \"layer3.1.bn2.num_batches_tracked\", \"layer3.1.conv3.weight\", \"layer3.1.bn3.weight\", \"layer3.1.bn3.bias\", \"layer3.1.bn3.running_mean\", \"layer3.1.bn3.running_var\", \"layer3.1.bn3.num_batches_tracked\", \"layer3.2.conv1.weight\", \"layer3.2.bn1.weight\", \"layer3.2.bn1.bias\", \"layer3.2.bn1.running_mean\", \"layer3.2.bn1.running_var\", \"layer3.2.bn1.num_batches_tracked\", \"layer3.2.conv2.weight\", \"layer3.2.bn2.weight\", \"layer3.2.bn2.bias\", \"layer3.2.bn2.running_mean\", \"layer3.2.bn2.running_var\", \"layer3.2.bn2.num_batches_tracked\", \"layer3.2.conv3.weight\", \"layer3.2.bn3.weight\", \"layer3.2.bn3.bias\", \"layer3.2.bn3.running_mean\", \"layer3.2.bn3.running_var\", \"layer3.2.bn3.num_batches_tracked\", \"layer3.3.conv1.weight\", \"layer3.3.bn1.weight\", \"layer3.3.bn1.bias\", \"layer3.3.bn1.running_mean\", \"layer3.3.bn1.running_var\", \"layer3.3.bn1.num_batches_tracked\", \"layer3.3.conv2.weight\", \"layer3.3.bn2.weight\", \"layer3.3.bn2.bias\", \"layer3.3.bn2.running_mean\", \"layer3.3.bn2.running_var\", \"layer3.3.bn2.num_batches_tracked\", \"layer3.3.conv3.weight\", \"layer3.3.bn3.weight\", \"layer3.3.bn3.bias\", \"layer3.3.bn3.running_mean\", \"layer3.3.bn3.running_var\", \"layer3.3.bn3.num_batches_tracked\", \"layer3.4.conv1.weight\", \"layer3.4.bn1.weight\", \"layer3.4.bn1.bias\", \"layer3.4.bn1.running_mean\", \"layer3.4.bn1.running_var\", \"layer3.4.bn1.num_batches_tracked\", \"layer3.4.conv2.weight\", \"layer3.4.bn2.weight\", \"layer3.4.bn2.bias\", \"layer3.4.bn2.running_mean\", \"layer3.4.bn2.running_var\", \"layer3.4.bn2.num_batches_tracked\", \"layer3.4.conv3.weight\", \"layer3.4.bn3.weight\", \"layer3.4.bn3.bias\", \"layer3.4.bn3.running_mean\", \"layer3.4.bn3.running_var\", \"layer3.4.bn3.num_batches_tracked\", \"layer3.5.conv1.weight\", \"layer3.5.bn1.weight\", \"layer3.5.bn1.bias\", \"layer3.5.bn1.running_mean\", \"layer3.5.bn1.running_var\", \"layer3.5.bn1.num_batches_tracked\", \"layer3.5.conv2.weight\", \"layer3.5.bn2.weight\", \"layer3.5.bn2.bias\", \"layer3.5.bn2.running_mean\", \"layer3.5.bn2.running_var\", \"layer3.5.bn2.num_batches_tracked\", \"layer3.5.conv3.weight\", \"layer3.5.bn3.weight\", \"layer3.5.bn3.bias\", \"layer3.5.bn3.running_mean\", \"layer3.5.bn3.running_var\", \"layer3.5.bn3.num_batches_tracked\", \"layer4.0.conv1.weight\", \"layer4.0.bn1.weight\", \"layer4.0.bn1.bias\", \"layer4.0.bn1.running_mean\", \"layer4.0.bn1.running_var\", \"layer4.0.bn1.num_batches_tracked\", \"layer4.0.conv2.weight\", \"layer4.0.bn2.weight\", \"layer4.0.bn2.bias\", \"layer4.0.bn2.running_mean\", \"layer4.0.bn2.running_var\", \"layer4.0.bn2.num_batches_tracked\", \"layer4.0.conv3.weight\", \"layer4.0.bn3.weight\", \"layer4.0.bn3.bias\", \"layer4.0.bn3.running_mean\", \"layer4.0.bn3.running_var\", \"layer4.0.bn3.num_batches_tracked\", \"layer4.0.shortcut.0.weight\", \"layer4.0.shortcut.1.weight\", \"layer4.0.shortcut.1.bias\", \"layer4.0.shortcut.1.running_mean\", \"layer4.0.shortcut.1.running_var\", \"layer4.0.shortcut.1.num_batches_tracked\", \"layer4.1.conv1.weight\", \"layer4.1.bn1.weight\", \"layer4.1.bn1.bias\", \"layer4.1.bn1.running_mean\", \"layer4.1.bn1.running_var\", \"layer4.1.bn1.num_batches_tracked\", \"layer4.1.conv2.weight\", \"layer4.1.bn2.weight\", \"layer4.1.bn2.bias\", \"layer4.1.bn2.running_mean\", \"layer4.1.bn2.running_var\", \"layer4.1.bn2.num_batches_tracked\", \"layer4.1.conv3.weight\", \"layer4.1.bn3.weight\", \"layer4.1.bn3.bias\", \"layer4.1.bn3.running_mean\", \"layer4.1.bn3.running_var\", \"layer4.1.bn3.num_batches_tracked\", \"layer4.2.conv1.weight\", \"layer4.2.bn1.weight\", \"layer4.2.bn1.bias\", \"layer4.2.bn1.running_mean\", \"layer4.2.bn1.running_var\", \"layer4.2.bn1.num_batches_tracked\", \"layer4.2.conv2.weight\", \"layer4.2.bn2.weight\", \"layer4.2.bn2.bias\", \"layer4.2.bn2.running_mean\", \"layer4.2.bn2.running_var\", \"layer4.2.bn2.num_batches_tracked\", \"layer4.2.conv3.weight\", \"layer4.2.bn3.weight\", \"layer4.2.bn3.bias\", \"layer4.2.bn3.running_mean\", \"layer4.2.bn3.running_var\", \"layer4.2.bn3.num_batches_tracked\", \"fc.weight\", \"fc.bias\". "
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "de53a77750427373"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
